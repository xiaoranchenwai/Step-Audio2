"""
FastRTC + Gradio å®æ—¶æµå¼å¯¹è¯ç³»ç»Ÿ for Step-Audio-2 (æ”¯æŒ VAD å’Œæ‰“æ–­)

ä½¿ç”¨ fastrtc.WebRTC å®ç°æŒç»­è¯­éŸ³è¾“å…¥ï¼Œæ— éœ€æ‰‹åŠ¨åœæ­¢å½•éŸ³:
- VAD è‡ªåŠ¨æ£€æµ‹è¯­éŸ³æ´»åŠ¨
- æ”¯æŒæ‰“æ–­ AI å›å¤
- å®æ—¶æµå¼æ–‡æœ¬å’ŒéŸ³é¢‘è¾“å‡º
- æŒç»­å¯¹è¯æ— éœ€ç‚¹å‡»

è¿è¡Œæ–¹å¼:
    pip install fastrtc librosa webrtcvad
    
    python step_audio2_streaming.py \
        --ssl-certfile cert.pem --ssl-keyfile key.pem --ssl-no-verify
    
    # æˆ–ä½¿ç”¨ share
    python step_audio2_streaming.py --share
"""

import argparse
import gradio as gr
import fastrtc
import numpy as np
from pathlib import Path
import tempfile
from datetime import datetime
import threading
import time
import wave
from typing import Generator, override
from queue import Queue, Empty

from stepaudio2vllm import StepAudio2
from token2wav import Token2wav

# VAD é…ç½®
try:
    import webrtcvad
    import librosa
    VAD_AVAILABLE = True
except ImportError as e:
    VAD_AVAILABLE = False
    print(f"è­¦å‘Š: VADä¾èµ–æœªå®‰è£… ({e})")
    print("å®‰è£…: pip install webrtcvad librosa")

CHUNK_SIZE = 25


class StepAudio2Service:
    """Step-Audio-2 æœåŠ¡å°è£…"""
    
    def __init__(self, api_url: str, model_name: str, token2wav_path: str, prompt_wav_path: str):
        self.model = StepAudio2(api_url, model_name)
        self.token2wav = Token2wav(token2wav_path)
        self.prompt_wav = prompt_wav_path
        self.tools = []
        self.generation_lock = threading.Lock()
        self.token2wav.set_stream_cache(self.prompt_wav)
        
    def save_audio_temp(self, sr: int, audio_data: np.ndarray) -> str:
        """ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶"""
        temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        wav_path = temp_wav.name
        temp_wav.close()
        
        with wave.open(wav_path, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)  # 16-bit
            wf.setframerate(sr)
            # ç¡®ä¿æ˜¯ int16 æ ¼å¼
            if audio_data.dtype == np.float32 or audio_data.dtype == np.float64:
                audio_int16 = (audio_data * 32767).astype(np.int16)
            else:
                audio_int16 = audio_data.astype(np.int16)
            wf.writeframes(audio_int16.tobytes())
        
        return wav_path
    
    def generate_response(
        self, 
        audio_input: tuple[int, np.ndarray],
        history: list,
        system_prompt: str = None
    ) -> Generator:
        """æµå¼ç”Ÿæˆå“åº”"""
        
        sr, audio_data = audio_input
        
        # æ›´æ–°å†å²è®°å½•
        user_msg = "ğŸ¤ [è¯­éŸ³æ¶ˆæ¯]"
        history.append({"role": "user", "content": user_msg})
        history.append({"role": "assistant", "content": ""})
        yield fastrtc.AdditionalOutputs(history)

        try:
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_audio_path = self.save_audio_temp(sr, audio_data)
            print(f"[Step-Audio-2] Processing audio from {temp_audio_path}")

            # æ„å»ºç³»ç»Ÿæç¤º
            if system_prompt is None:
                system_prompt = (
                    f"ä½ çš„åå­—å«åšå°è·ƒï¼Œæ˜¯ç”±é˜¶è·ƒæ˜Ÿè¾°å…¬å¸è®­ç»ƒå‡ºæ¥çš„è¯­éŸ³å¤§æ¨¡å‹ã€‚"
                    f"ä»Šå¤©æ˜¯{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}ã€‚"
                    f"è¯·ç”¨é»˜è®¤å¥³å£°ä¸ç”¨æˆ·äº¤æµï¼Œå›å¤è¦ç®€æ´å‹å¥½ã€‚"
                )
            
            # æ„å»ºå¯¹è¯å†å²
            step_history = [
                {"role": "system", "content": system_prompt},
                {"role": "human", "content": [{"type": "audio", "audio": temp_audio_path}]},
                {"role": "assistant", "content": "<tts_start>", "eot": False}
            ]

            # åˆå§‹åŒ– token2wav ç¼“å­˜
            #self.token2wav.set_stream_cache(self.prompt_wav)
            
            # ç”¨äºç´¯ç§¯ç»“æœ
            full_text = ""
            audio_chunks = []
            pcm_buffer = []
            
            with self.generation_lock:
                # æµå¼ç”Ÿæˆ
                for line, text, audio in self.model.stream(
                    step_history,
                    tools=self.tools,
                    max_tokens=4096,
                    repetition_penalty=1.05,
                    top_p=0.9,
                    temperature=0.7
                ):
                    # å¤„ç†æ–‡æœ¬æµ
                    if text:
                        full_text += text
                        history[-1]["content"] = full_text
                        yield fastrtc.AdditionalOutputs(history)
                    
                    # å¤„ç†éŸ³é¢‘æµ
                    if audio:
                        pcm_buffer += audio
                        
                        # å½“ç¼“å†²åŒºè¶³å¤Ÿå¤§æ—¶ï¼Œç”ŸæˆéŸ³é¢‘
                        if len(pcm_buffer) >= CHUNK_SIZE + self.token2wav.flow.pre_lookahead_len:
                            chunk_to_decode = pcm_buffer[:CHUNK_SIZE + self.token2wav.flow.pre_lookahead_len]
                            wav_chunk = self.token2wav.stream(
                                chunk_to_decode,
                                prompt_wav=self.prompt_wav,
                                last_chunk=False
                            )
                            
                            # å°† PCM bytes è½¬æ¢ä¸º numpy array (int16)
                            audio_np = np.frombuffer(wav_chunk, dtype=np.int16)
                            audio_chunks.append(audio_np)
                            pcm_buffer = pcm_buffer[CHUNK_SIZE:]
                            
                            # è¾“å‡ºç´¯ç§¯çš„éŸ³é¢‘ (int16 æ ¼å¼ï¼Œ24kHz)
                            full_audio = np.concatenate(audio_chunks)
                            yield (24000, full_audio)
                
                # å¤„ç†å‰©ä½™çš„éŸ³é¢‘ç¼“å†²
                if pcm_buffer:
                    wav_chunk = self.token2wav.stream(
                        pcm_buffer,
                        prompt_wav=self.prompt_wav,
                        last_chunk=True
                    )
                    audio_np = np.frombuffer(wav_chunk, dtype=np.int16)
                    audio_chunks.append(audio_np)
                    full_audio = np.concatenate(audio_chunks)
                    yield (24000, full_audio)

            # æœ€ç»ˆè¾“å‡º
            yield fastrtc.AdditionalOutputs(history)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                Path(temp_audio_path).unlink()
            except:
                pass

        except GeneratorExit:
            print("[Step-Audio-2] Generation interrupted by VAD")
            raise
        except Exception as e:
            print(f"[Step-Audio-2] Error: {e}")
            import traceback
            traceback.print_exc()
            history[-1]["content"] += f"\n[é”™è¯¯: {e}]"
            yield fastrtc.AdditionalOutputs(history)


class RealTimeVAD:
    """å®æ—¶ VAD å¤„ç†å™¨ - ä½¿ç”¨æ–°çš„å®ç°æ–¹å¼"""
    
    def __init__(self, src_sr=24000, vad_sr=16000, frame_duration_ms=30, mode=3):
        self.src_sr = src_sr
        self.vad_sr = vad_sr
        self.frame_duration_ms = frame_duration_ms
        
        if not VAD_AVAILABLE:
            raise ImportError("webrtcvad å’Œ librosa å¿…é¡»å®‰è£…")
        
        self.vad = webrtcvad.Vad(mode)
        
        # è®¡ç®—æ¯å¸§æ ·æœ¬æ•°
        self.samples_per_frame = int(vad_sr * frame_duration_ms / 1000)
        
        # é‡é‡‡æ ·åéŸ³é¢‘çš„ç§¯ç´¯ç¼“å†²åŒº
        self.vad_buffer = np.array([], dtype=np.int16)
        
        # çŠ¶æ€æœºç›¸å…³
        self.audio_buffer = []  # å­˜å‚¨åŸå§‹é‡‡æ ·ç‡çš„éŸ³é¢‘
        self.is_speaking = False
        self.silence_frames = 0
        self.speech_frames = 0
        self.silence_threshold = 5
        self.speech_threshold = 30
        self.is_ai_speaking = False
        self.frame_count = 0
    
    class VADEvent:
        def __init__(self):
            self.interrupt_signal = False
            self.full_audio: tuple[int, np.ndarray] | None = None
    
    def process_chunk(self, audio_chunk: bytes):
        """
        å¤„ç†ä¸€æ®µå®æ—¶éŸ³é¢‘ chunkï¼Œè¿”å›è¿™ä¸€æ®µä¸­æ‰€æœ‰å¸§çš„ VAD ç»“æœ
        """
        if not audio_chunk:
            return []
        
        audio_data = np.frombuffer(audio_chunk, dtype=np.int16)
        
        # è½¬æ¢ä¸º float32
        float_audio = audio_data.astype(np.float32) / 32768.0
        
        # é‡é‡‡æ ·
        if self.src_sr != self.vad_sr:
            resampled = librosa.resample(float_audio, orig_sr=self.src_sr, target_sr=self.vad_sr)
        else:
            resampled = float_audio
        
        # è½¬å› int16
        resampled_int16 = np.clip(resampled * 32767.0, -32768, 32767).round().astype(np.int16)
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.vad_buffer = np.concatenate((self.vad_buffer, resampled_int16))
        
        results_this_chunk = []
        while len(self.vad_buffer) >= self.samples_per_frame:
            frame = self.vad_buffer[:self.samples_per_frame]
            frame_bytes = frame.tobytes()
            
            try:
                is_speech = self.vad.is_speech(frame_bytes, self.vad_sr)
            except Exception as e:
                print(f"[VAD] æ£€æµ‹é”™è¯¯: {e}")
                is_speech = False
            
            results_this_chunk.append(is_speech)
            self.vad_buffer = self.vad_buffer[self.samples_per_frame:]
        
        return results_this_chunk
    
    def process(self, audio_data: np.ndarray):
        """å¤„ç†éŸ³é¢‘å¸§å¹¶äº§ç”Ÿäº‹ä»¶"""
        self.frame_count += 1
        event = self.VADEvent()
        
        # å°† numpy array è½¬æ¢ä¸º bytes
        if audio_data.dtype == np.float32 or audio_data.dtype == np.float64:
            audio_int16 = (audio_data * 32767).astype(np.int16)
        else:
            audio_int16 = audio_data.astype(np.int16)
        audio_bytes = audio_int16.tobytes()
        
        # ä½¿ç”¨æ–°çš„ VAD å¤„ç†
        vad_results = self.process_chunk(audio_bytes)
        
        # å¦‚æœæ²¡æœ‰å®Œæ•´çš„å¸§ï¼Œç›´æ¥è¿”å›
        if not vad_results:
            yield event
            return
        
        # ä½¿ç”¨æœ€åä¸€å¸§çš„ç»“æœä½œä¸ºå½“å‰çŠ¶æ€ï¼ˆæˆ–è€…å¯ä»¥ä½¿ç”¨å¤šæ•°æŠ•ç¥¨ï¼‰
        is_speech = vad_results[-1]  # æˆ–è€…: is_speech = sum(vad_results) > len(vad_results) / 2
        
        # è¯¦ç»†çš„çŠ¶æ€æ—¥å¿—
        if self.frame_count % 20 == 0:
            print(f"[VAD] å¸§{self.frame_count} | is_speaking:{self.is_speaking} | speech_frames:{self.speech_frames} | silence_frames:{self.silence_frames} | buffer_size:{len(self.audio_buffer)} | is_speech:{is_speech}")
        
        # å¦‚æœæ­£åœ¨ AI è¯´è¯æ—¶æ£€æµ‹åˆ°äººå£°ï¼Œè§¦å‘æ‰“æ–­
        if self.is_ai_speaking and is_speech:
            print("[VAD] âš ï¸ æ£€æµ‹åˆ°æ‰“æ–­ä¿¡å·ï¼")
            event.interrupt_signal = True
            self.is_ai_speaking = False
            self.audio_buffer = []
            self.is_speaking = False
            self.speech_frames = 0
            self.silence_frames = 0
            yield event
            return
        
        # VAD çŠ¶æ€æœº
        if is_speech:
            self.speech_frames += 1
            self.silence_frames = 0
            
            if not self.is_speaking and self.speech_frames >= self.speech_threshold:
                print(f"[VAD] âœ…âœ…âœ… å¼€å§‹è¯´è¯ï¼(è¯­éŸ³å¸§: {self.speech_frames})")
                self.is_speaking = True
                self.audio_buffer = []
            
            if self.is_speaking:
                self.audio_buffer.append(audio_data)
                if len(self.audio_buffer) % 50 == 0:
                    duration = len(self.audio_buffer) * len(audio_data) / self.src_sr
                    print(f"[VAD] ğŸ“ å½•éŸ³ä¸­... ç¼“å†²åŒº: {len(self.audio_buffer)} å¸§ ({duration:.2f}ç§’)")
        else:
            self.silence_frames += 1
            self.speech_frames = 0
            
            if self.is_speaking:
                self.audio_buffer.append(audio_data)
                
                print(f"[VAD] ğŸ”‡ é™éŸ³ä¸­: {self.silence_frames}/{self.silence_threshold} (ç¼“å†²åŒº: {len(self.audio_buffer)} å¸§)")
                
                if self.silence_frames >= self.silence_threshold:
                    print(f"[VAD] âœ…âœ…âœ… è¯´è¯ç»“æŸï¼è§¦å‘ç”Ÿæˆï¼")
                    print(f"[VAD] æœ€ç»ˆç¼“å†²åŒº: {len(self.audio_buffer)} å¸§")
                    self.is_speaking = False
                    
                    # åˆå¹¶éŸ³é¢‘
                    full_audio = np.concatenate(self.audio_buffer)
                    audio_duration = len(full_audio) / self.src_sr
                    print(f"[VAD] ğŸµ éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} ç§’, é‡‡æ ·ç‚¹: {len(full_audio)}")
                    
                    event.full_audio = (self.src_sr, full_audio)
                    self.audio_buffer = []
                    self.silence_frames = 0
                    self.speech_frames = 0
                    
                    # æ ‡è®° AI å¼€å§‹è¯´è¯
                    self.is_ai_speaking = True
                    
                    print(f"[VAD] ğŸš€ å‡†å¤‡è¿”å› full_audio äº‹ä»¶")
                    yield event
                    return
        
        yield event


type StreamerGenerator = Generator[fastrtc.tracks.EmitType, None, None]


class VADStreamHandler(fastrtc.StreamHandler):
    """FastRTC Stream Handler å¸¦ VAD æ”¯æŒ"""
    
    def __init__(
        self,
        step_service: StepAudio2Service,
        input_sample_rate: int = 24000,
    ):
        super().__init__(
            expected_layout="mono",
            output_sample_rate=24000,
            output_frame_size=None,
            input_sample_rate=input_sample_rate,
        )
        self.step_service = step_service
        self.realtime_vad = RealTimeVAD(src_sr=input_sample_rate)
        self.generator: StreamerGenerator | None = None
        self.latest_history = []
        self.close_requested = threading.Event()

    @override
    def emit(self) -> fastrtc.tracks.EmitType:
        """å‘é€æ•°æ®åˆ°å‰ç«¯"""
        if self.close_requested.is_set():
            if self.generator:
                print("[Handler] å…³é—­ç”Ÿæˆå™¨ï¼ˆæ‰“æ–­ï¼‰")
                self.generator.close()
                self.generator = None
            self.close_requested.clear()
            return None
        
        if self.generator is None:
            return None

        try:
            return next(self.generator)
        except StopIteration:
            print("[Handler] ç”Ÿæˆå®Œæˆ")
            self.generator = None
            # é€šçŸ¥ VAD AI å·²åœæ­¢è¯´è¯
            self.realtime_vad.is_ai_speaking = False
            return None
        except Exception as e:
            print(f"[Handler] ç”Ÿæˆå™¨é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            self.generator = None
            self.realtime_vad.is_ai_speaking = False
            return None

    @override
    def receive(self, frame: tuple[int, np.ndarray]):
        """æ¥æ”¶æ¥è‡ªå‰ç«¯çš„éŸ³é¢‘å¸§"""
        sr, audio_data = frame
        
        # è°ƒè¯•ï¼šæ‰“å°éŸ³é¢‘æ•°æ®ä¿¡æ¯
        if hasattr(self, '_frame_count'):
            self._frame_count += 1
        else:
            self._frame_count = 0
            print("[Handler] ========== åˆå§‹åŒ– ==========")
            
        if self._frame_count % 100 == 0:
            print(f"\n[Handler] === å¸§ {self._frame_count} ===")
            print(f"  é‡‡æ ·ç‡: {sr}, å½¢çŠ¶: {audio_data.shape}, ç±»å‹: {audio_data.dtype}")
            print(f"  èŒƒå›´: [{audio_data.min():.6f}, {audio_data.max():.6f}]")
            print(f"  èƒ½é‡: {np.abs(audio_data).mean():.6f}")
        
        event_count = 0
        for event in self.realtime_vad.process(audio_data):
            event_count += 1
            
            if event.interrupt_signal:
                print("[Handler] âš ï¸âš ï¸âš ï¸ >>> æ£€æµ‹åˆ°æ‰“æ–­ä¿¡å· <<<")
                self.close_requested.set()
                self.clear_queue()

            if event.full_audio is not None:
                print("\n" + "="*70)
                print("[Handler] ğŸ‰ğŸ‰ğŸ‰ >>> æ¥æ”¶åˆ°å®Œæ•´éŸ³é¢‘ï¼<<<")
                print("="*70)
                
                sr_full, audio_full = event.full_audio
                print(f"[Handler] å®Œæ•´éŸ³é¢‘ä¿¡æ¯:")
                print(f"  - é‡‡æ ·ç‡: {sr_full} Hz")
                print(f"  - é‡‡æ ·ç‚¹æ•°: {len(audio_full)}")
                print(f"  - æ—¶é•¿: {len(audio_full) / sr_full:.2f} ç§’")
                print(f"  - æ•°æ®ç±»å‹: {audio_full.dtype}")
                
                if self.close_requested.is_set():
                    print("[Handler] æ¸…é™¤æ‰“æ–­æ ‡å¿—")
                    self.close_requested.clear()
                
                # åŒæ­¥è·å–æœ€æ–°çš„å†å²è®°å½•
                print("[Handler] ç­‰å¾…å‚æ•°åŒæ­¥...")
                self.wait_for_args()
                if len(self.latest_args) > 0:
                    self.latest_history = self.latest_args[-1]
                    print(f"[Handler] âœ… å¯¹è¯å†å²é•¿åº¦: {len(self.latest_history)}")
                else:
                    print("[Handler] âš ï¸ æ²¡æœ‰å†å²è®°å½•ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
                    self.latest_history = []
                
                # ä½¿ç”¨ Step-Audio-2 æœåŠ¡ç”Ÿæˆå“åº”
                print("[Handler] ğŸš€ æ­£åœ¨è°ƒç”¨ generate_response...")
                try:
                    self.generator = self.step_service.generate_response(
                        event.full_audio,
                        self.latest_history
                    )
                    print("[Handler] âœ… generate_response å·²å¯åŠ¨ï¼Œgenerator:", self.generator)
                except Exception as e:
                    print(f"[Handler] âŒ generate_response å¯åŠ¨å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
                print("="*70 + "\n")
        
        if self._frame_count % 100 == 0 and event_count > 0:
            print(f"[Handler] å¤„ç†äº† {event_count} ä¸ªäº‹ä»¶")

    @override
    def copy(self):
        """åˆ›å»ºæ–°çš„å¤„ç†å™¨å‰¯æœ¬"""
        return VADStreamHandler(
            step_service=self.step_service,
            input_sample_rate=self.input_sample_rate,
        )


def build_interface(service: StepAudio2Service) -> gr.Blocks:
    """æ„å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(title="Step-Audio-2 å®æ—¶å¯¹è¯ç³»ç»Ÿ") as demo:
        gr.Markdown("### ğŸ™ï¸ Step-Audio-2 å®æ—¶è¯­éŸ³å¯¹è¯ (æ”¯æŒ VAD å’Œæ‰“æ–­)")
        gr.Markdown(
            "**è¯´æ˜**: å¼€å§‹è¯´è¯ä¼šè‡ªåŠ¨è§¦å‘ VADã€‚åœ¨ AI å›å¤è¿‡ç¨‹ä¸­è¯´è¯ï¼Œä¼šè§¦å‘æ‰“æ–­ä¿¡å·å¹¶åœæ­¢å½“å‰å›å¤ã€‚\n\n"
            "âœ¨ **æ— éœ€ç‚¹å‡»** - ç›´æ¥è¯´è¯å³å¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹è¯­éŸ³æ´»åŠ¨"
        )

        with gr.Row():
            with gr.Column(scale=1):
                webrtc = fastrtc.WebRTC(
                    label="ğŸ™ï¸ è¯­éŸ³é€šè¯",
                    mode="send-receive",
                    modality="audio",
                    rtc_configuration={
                        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
                    }
                )
                
                with gr.Row():
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å†å²", size="sm")
                    manual_trigger_btn = gr.Button("âš¡ æ‰‹åŠ¨è§¦å‘ï¼ˆæµ‹è¯•ç”¨ï¼‰", size="sm", variant="secondary")
                
                gr.Markdown("""
                **è°ƒè¯•æ¨¡å¼**: å¦‚æœ VAD è‡ªåŠ¨è§¦å‘æœ‰é—®é¢˜ï¼Œå¯ä»¥ï¼š
                1. è¯´è¯åç‚¹å‡»"æ‰‹åŠ¨è§¦å‘"æŒ‰é’®å¼ºåˆ¶ç»“æŸå½•éŸ³
                2. æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—äº†è§£ VAD çŠ¶æ€
                """)
            
            with gr.Column(scale=1):
                chatbot = gr.Chatbot(
                    label="ğŸ’¬ å¯¹è¯è®°å½•",
                    height=500,
                    type="messages"
                )

        chat_history = gr.State([])

        # åˆå§‹åŒ– Handler
        handler = VADStreamHandler(
            step_service=service,
            input_sample_rate=24000
        )

        # ç»‘å®š Stream
        webrtc.stream(
            handler,
            inputs=[webrtc, chat_history],
            outputs=[webrtc],
            time_limit=3600  # 1å°æ—¶
        )

        # ç›‘å¬é¢å¤–è¾“å‡ºï¼ˆå¯¹è¯å†å²æ›´æ–°ï¼‰
        webrtc.on_additional_outputs(
            lambda h: h,
            outputs=[chatbot],
            queue=False,
            show_progress="hidden"
        )

        # æ¸…é™¤å†å²
        clear_btn.click(
            lambda: ([], []),
            outputs=[chat_history, chatbot]
        )
        
        # æ‰‹åŠ¨è§¦å‘ï¼ˆæµ‹è¯•ç”¨ï¼‰
        def manual_trigger():
            """æ‰‹åŠ¨è§¦å‘å½•éŸ³ç»“æŸ"""
            print("\n" + "="*70)
            print("[æ‰‹åŠ¨è§¦å‘] ç”¨æˆ·ç‚¹å‡»äº†æ‰‹åŠ¨è§¦å‘æŒ‰é’®")
            print("="*70)
            
            if handler.realtime_vad.is_speaking and len(handler.realtime_vad.audio_buffer) > 0:
                print(f"[æ‰‹åŠ¨è§¦å‘] å½“å‰æ­£åœ¨è¯´è¯ï¼Œç¼“å†²åŒºæœ‰ {len(handler.realtime_vad.audio_buffer)} å¸§")
                print("[æ‰‹åŠ¨è§¦å‘] å¼ºåˆ¶è®¾ç½®é™éŸ³å¸§æ•°ä»¥è§¦å‘ç»“æŸ")
                handler.realtime_vad.silence_frames = handler.realtime_vad.silence_threshold
                return "âœ… å·²æ‰‹åŠ¨è§¦å‘ï¼Œè¯·ç­‰å¾…å¤„ç†..."
            else:
                print(f"[æ‰‹åŠ¨è§¦å‘] å½“å‰æœªåœ¨è¯´è¯çŠ¶æ€ (is_speaking={handler.realtime_vad.is_speaking})")
                print(f"[æ‰‹åŠ¨è§¦å‘] ç¼“å†²åŒºå¤§å°: {len(handler.realtime_vad.audio_buffer)}")
                return "âš ï¸ æœªæ£€æµ‹åˆ°å½•éŸ³æ•°æ®"
        
        manual_trigger_btn.click(
            fn=manual_trigger,
            outputs=[]
        )
        
        gr.Markdown("""
        ---
        ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
        1. **ç‚¹å‡»"å¼€å§‹"** å¯åŠ¨è¯­éŸ³é€šè¯
        2. **ç›´æ¥è¯´è¯** - ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹è¯­éŸ³å¼€å§‹å’Œç»“æŸ
        3. **ç­‰å¾…å›å¤** - AI ä¼šå®æ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³
        4. **éšæ—¶æ‰“æ–­** - åœ¨ AI è¯´è¯æ—¶å¼€å§‹è¯´è¯ï¼Œä¼šè‡ªåŠ¨æ‰“æ–­
        5. **ç»§ç»­å¯¹è¯** - æ‰“æ–­åå¯ä»¥ç»§ç»­è¯´è¯æˆ–ç­‰å¾…
        
        ### ğŸ”§ æŠ€æœ¯ç‰¹æ€§
        - âœ… VAD è‡ªåŠ¨è¯­éŸ³æ£€æµ‹ï¼ˆåŸºäº webrtcvad + librosaï¼‰
        - âœ… æŒç»­é€šè¯æ— éœ€ç‚¹å‡»
        - âœ… æ”¯æŒæ‰“æ–­åŠŸèƒ½
        - âœ… æµå¼æ–‡æœ¬ç”Ÿæˆ
        - âœ… æµå¼è¯­éŸ³åˆæˆ
        - âœ… ä½å»¶è¿Ÿå®æ—¶äº¤äº’
        
        ### âš™ï¸ VAD å‚æ•°
        - æºé‡‡æ ·ç‡: 24kHz
        - VAD é‡‡æ ·ç‡: 16kHz
        - æ¿€è¿›åº¦: 3 (0-3)
        - è¯­éŸ³é˜ˆå€¼: 3 å¸§
        - é™éŸ³é˜ˆå€¼: 15 å¸§
        - å¸§æ—¶é•¿: 30ms
        
        ### ğŸ› è°ƒè¯•æç¤º
        - æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—ï¼Œäº†è§£ VAD æ£€æµ‹çŠ¶æ€
        - å¦‚æœä¸€ç›´ä¸è§¦å‘ï¼Œå¯èƒ½æ˜¯éº¦å…‹é£éŸ³é‡å¤ªå°
        - å»ºè®®é è¿‘éº¦å…‹é£ï¼Œæ¸…æ™°è¯´è¯
        """)
    
    return demo


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Step-Audio-2 å®æ—¶æµå¼å¯¹è¯ç³»ç»Ÿ")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--api-url",
        default="http://10.250.2.26:8005/v1/chat/completions",
        help="Step-Audio-2 API åœ°å€"
    )
    parser.add_argument(
        "--model-name",
        default="step-audio-2-mini",
        help="æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--token2wav-path",
        default="/home/user/cx/new_data/Step-Audio2/token2wav",
        help="Token2Wav æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--prompt-wav-path",
        default="/home/user/cx/new_data/Step-Audio2/assets/default_female.wav",
        help="æç¤ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„"
    )
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument(
        "--server-name",
        default="0.0.0.0",
        help="æœåŠ¡å™¨åœ°å€"
    )
    parser.add_argument(
        "--server-port",
        type=int,
        default=7862,
        help="æœåŠ¡å™¨ç«¯å£"
    )
    parser.add_argument(
        "--share",
        action="store_true",
        help="åˆ›å»º Gradio å…¬å…±åˆ†äº«é“¾æ¥"
    )
    
    # SSL é…ç½®
    parser.add_argument(
        "--ssl-certfile",
        default="/home/user/cx/new_data/MiMo-Audio/cert.pem",
        help="SSL è¯ä¹¦æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--ssl-keyfile",
        default="/home/user/cx/new_data/MiMo-Audio/key.pem",
        help="SSL ç§é’¥æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--ssl-keyfile-password",
        default=None,
        help="SSL ç§é’¥å¯†ç "
    )
    parser.add_argument(
        "--ssl-no-verify",
        default=True,
        help="ç¦ç”¨ SSL è¯ä¹¦éªŒè¯"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("=" * 70)
    print("Step-Audio-2 å®æ—¶æµå¼å¯¹è¯ç³»ç»Ÿ (FastRTC + VAD)")
    print("=" * 70)
    
    print(f"\nğŸ“¡ æ¨¡å‹é…ç½®:")
    print(f"  - API åœ°å€: {args.api_url}")
    print(f"  - æ¨¡å‹åç§°: {args.model_name}")
    print(f"  - Token2Wav: {args.token2wav_path}")
    print(f"  - æç¤ºéŸ³é¢‘: {args.prompt_wav_path}")
    
    print(f"\nğŸŒ æœåŠ¡å™¨é…ç½®:")
    print(f"  - åœ°å€: {args.server_name}")
    print(f"  - ç«¯å£: {args.server_port}")
    print(f"  - åˆ†äº«é“¾æ¥: {'å¯ç”¨' if args.share else 'ç¦ç”¨'}")
    
    if not VAD_AVAILABLE:
        print("\nâš ï¸  è­¦å‘Š: webrtcvad æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„èƒ½é‡æ£€æµ‹")
        print("   æ¨èå®‰è£…: pip install webrtcvad")
    
    # SSL é…ç½®æ£€æŸ¥
    ssl_enabled = False
    if args.ssl_certfile and args.ssl_keyfile:
        print(f"\nğŸ”’ SSL/HTTPS é…ç½®:")
        print(f"  - è¯ä¹¦æ–‡ä»¶: {args.ssl_certfile}")
        print(f"  - ç§é’¥æ–‡ä»¶: {args.ssl_keyfile}")
        print(f"  - è¯ä¹¦éªŒè¯: {'ç¦ç”¨' if args.ssl_no_verify else 'å¯ç”¨'}")
        ssl_enabled = True
    
    # åˆå§‹åŒ–æœåŠ¡
    print("\nğŸ”§ åˆå§‹åŒ–æœåŠ¡...")
    try:
        service = StepAudio2Service(
            api_url=args.api_url,
            model_name=args.model_name,
            token2wav_path=args.token2wav_path,
            prompt_wav_path=args.prompt_wav_path,
        )
        print("âœ… æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æ„å»ºç•Œé¢
    print("ğŸ¨ æ„å»ºç•Œé¢...")
    demo = build_interface(service)
    
    # å¯åŠ¨æœåŠ¡
    print("\nğŸš€ å¯åŠ¨æœåŠ¡...")
    
    protocol = "https" if ssl_enabled else "http"
    
    if args.server_name == "0.0.0.0":
        print(f"   è®¿é—®åœ°å€: {protocol}://localhost:{args.server_port}")
        print(f"   å±€åŸŸç½‘è®¿é—®: {protocol}://<your-ip>:{args.server_port}")
    else:
        print(f"   è®¿é—®åœ°å€: {protocol}://{args.server_name}:{args.server_port}")
    
    if args.share:
        print("   Gradio åˆ†äº«é“¾æ¥: å¯åŠ¨åè‡ªåŠ¨ç”Ÿæˆ...")
    
    print("\nâš ï¸  é‡è¦æç¤º:")
    print("   - æµè§ˆå™¨éœ€è¦éº¦å…‹é£æƒé™")
    print("   - WebRTC éœ€è¦å®‰å…¨ä¸Šä¸‹æ–‡ï¼ˆHTTPS æˆ– localhostï¼‰")
    print("   - å»ºè®®ä½¿ç”¨ Chrome/Edge æµè§ˆå™¨")
    print("   - ç‚¹å‡»'å¼€å§‹'åç›´æ¥è¯´è¯ï¼Œæ— éœ€å…¶ä»–æ“ä½œ")
    print("\nğŸ“¦ ä¾èµ–æ£€æŸ¥:")
    print("   - pip install fastrtc")
    print("   - pip install webrtcvad (å¯é€‰ï¼Œæä¾›æ›´å¥½çš„ VAD)")
    
    print("\n" + "=" * 70)
    
    # å¯åŠ¨ Gradio
    try:
        launch_kwargs = {
            "server_name": args.server_name,
            "server_port": args.server_port,
            "share": args.share,
            "debug": True,
        }
        
        if ssl_enabled:
            launch_kwargs.update({
                "ssl_certfile": args.ssl_certfile,
                "ssl_keyfile": args.ssl_keyfile,
                "ssl_keyfile_password": args.ssl_keyfile_password,
                "ssl_verify": not args.ssl_no_verify,
            })
        
        demo.launch(**launch_kwargs)
        
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()